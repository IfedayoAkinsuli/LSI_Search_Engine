{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LSI For Text Retrieval\n",
    "___________________________________________________________"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (4.3.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (from gensim) (6.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ifeda\\pycharmprojects\\lsi_search_engine\\venv\\lib\\site-packages (from rank-bm25) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install rank-bm25\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data\n",
    "The model is based on three texts, the aim is to use LSI to retrieve the most relevant text from the collection of texts.\n",
    "1. Harry Potter and the Philosophers Stone\n",
    "2. The Fellowship of the Ring\n",
    "3. Pride and Prejudice\n",
    "\n",
    "The texts themselves are available on github and were imported in  using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_list = [\n",
    "    \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher's%20Stone.txt\",\n",
    "    \"https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/01%20-%20The%20Fellowship%20Of%20The%20Ring.txt\",\n",
    "    \"https://raw.githubusercontent.com/laumann/ds/master/hashing/books/jane-austen-pride-prejudice.txt\"\n",
    "    ]\n",
    "\n",
    "documents = [\"Harry Potter and the Philosopher's Stone\",\"Fellowship of the Ring\",\"Pride and Prejudice\"]\n",
    "text = []\n",
    "\n",
    "for _,i in enumerate(url_list):\n",
    "    req = requests.get(i)\n",
    "    text.append(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing\n",
    "The text files need to be processed in order for use in the Search Engine. The processing methods implemented are as follows:\n",
    "- Tokenization\n",
    "- Lower casing\n",
    "- Stop word removal\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    lower_filter = [w.lower() for w in text]\n",
    "    filtered_text = []\n",
    "\n",
    "    filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "    stop_words = stopwords.words('english') + ['j','page','k','said','rowling','quot','back','mr','mrs']\n",
    "\n",
    "    for words in lower_filter:\n",
    "        if words not in stop_words:\n",
    "            filtered_text.append(words)\n",
    "\n",
    "    filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    filtered_text = [lem.lemmatize(w) for w in filtered_text]\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed = [preprocessing(i) for _,i in enumerate(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Processed Document Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['boy',\n 'lived',\n 'dursley',\n 'number',\n 'four',\n 'privet',\n 'drive',\n 'proud',\n 'say',\n 'perfectly']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Queries\n",
    "\n",
    "Below are example user queries written in natural language terms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "queries = [\"Who is the ghost that haunts the Hogwarts girls' bathroom?\",\"Which animal is the Patronus of Severus Snape?\",\"Who was the Defense Against the Dark Arts teacher in Harry's third year at Hogwarts\",\"Who was the founder of Slytherin House and what was his gifts\",\"Who is darcy?\",\"Where can I find the one ring to rule them all\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query Processor\n",
    "The Queries also have to be processed before being mapped into the semantic space. The preprocessing of the query involve:\n",
    "- Tokenization\n",
    "- Lower Casing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def query_processing(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    processed_query = tokenizer.tokenize(text)\n",
    "    processed_query = [w.lower() for w in processed_query]\n",
    "    return processed_query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example processed queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['who',\n 'is',\n 'the',\n 'ghost',\n 'that',\n 'haunts',\n 'the',\n 'hogwarts',\n 'girls',\n 'bathroom']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_queries = [query_processing(i) for _,i in enumerate(queries)]\n",
    "processed_queries[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Corpus and Indexing to form Doc-Term-Matrix\n",
    "Using the gensim library the documents were used to generate a bag of words. These words were then used to generate a term document matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "X = Dictionary(processed)\n",
    "\n",
    "doc_term_matrix = [X.doc2bow(doc) for doc in processed]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing the LSI Model\n",
    "The LSI model was created as below with the number of topics representing each text from the collection. The model carries out an SVD calculation to generate the term concept space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsi_model = LsiModel(corpus=doc_term_matrix,num_topics=3,id2word=X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 Topics from 3 distinct texts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0,\n  '0.251*\"frodo\" + 0.177*\"could\" + 0.160*\"would\" + 0.156*\"one\" + 0.152*\"harry\"'),\n (1,\n  '-0.743*\"harry\" + -0.205*\"potter\" + 0.193*\"frodo\" + -0.191*\"ron\" + -0.165*\"hagrid\"'),\n (2,\n  '-0.333*\"elizabeth\" + 0.233*\"frodo\" + -0.219*\"darcy\" + -0.175*\"bennet\" + -0.162*\"could\"')]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_words=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert the Query into LSI Space\n",
    "The queries are still in naturla language terms and as such they have to be mapped to be represented in the concept term space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.010196347685162956), (1, -0.046187303820103366), (2, 0.009154755902625687)]\n"
     ]
    }
   ],
   "source": [
    "indexed_query = [Dictionary(processed).doc2bow(j) for j in processed_queries]\n",
    "vector_query = [lsi_model[a] for a in indexed_query]\n",
    "print(vector_query[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matrix Similarity\n",
    "The method for document retrieval is based on the cosine similarity between the query and the documents mapped in the concept space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lsi_model[doc_term_matrix])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Ranking\n",
    "The following code cells perform the cosine similarity calculations and returns the reevant document as well as ouput the similarity score between the queries and the documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def doc_ranking_score(similarities):\n",
    "    score = []\n",
    "\n",
    "    for i in similarities:\n",
    "        t = np.argsort(i,-1)[::-1]\n",
    "        sol = i[t]\n",
    "        print(f'The most relevant books and corresponding scores are {list(t+1)}  {sol}')\n",
    "        score.append(t)\n",
    "\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant books and corresponding scores are [1, 2, 3]  [ 9.3354422e-01  1.8608503e-02 -1.7413836e-08]\n",
      "The most relevant books and corresponding scores are [1, 2, 3]  [0.93685794 0.0285731  0.0054644 ]\n",
      "The most relevant books and corresponding scores are [1, 2, 3]  [0.9659396  0.16968174 0.03957374]\n",
      "The most relevant books and corresponding scores are [1, 3, 2]  [0.848066  0.7509906 0.5083917]\n",
      "The most relevant books and corresponding scores are [3, 1, 2]  [ 8.5409087e-01 -4.9142024e-09 -1.3282470e-08]\n",
      "The most relevant books and corresponding scores are [2, 3, 1]  [0.98938537 0.5503045  0.4754797 ]\n"
     ]
    }
   ],
   "source": [
    "similarities = index[vector_query]\n",
    "rel = doc_ranking_score(similarities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate key Metrics\n",
    "As the collection is small the document retrieved will only be the most relevant 1 document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def retrieval(similarities,documents):\n",
    "    t = np.argmax(similarities)\n",
    "\n",
    "    return  documents[int(t)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Philosopher's Stone\n"
     ]
    }
   ],
   "source": [
    "similarities = index[vector_query[0]]\n",
    "output = retrieval(similarities,documents)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Precision is 100.0 and the accuracy is 100.0\n"
     ]
    }
   ],
   "source": [
    "def metrics():\n",
    "    precision = 100 * (1/1) #Given that the above is an argument max the relevancy was also prejudged hence the scores\n",
    "    recall = 100 * (1/1)\n",
    "\n",
    "    return precision,recall\n",
    "\n",
    "precision,recall = metrics()\n",
    "\n",
    "print(f'The Precision is {precision} and the accuracy is {recall}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Simple BM25 Implementation as a Benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25 = BM25Okapi(processed)\n",
    "doc_scores = [bm25.get_scores(i) for i in processed_queries]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant books and corresponding scores are [1, 3, 2]  [ 1.22278439  0.         -0.01725772]\n",
      "The most relevant books and corresponding scores are [1, 3, 2]  [ 2.29452938 -0.00861595 -0.01652686]\n",
      "The most relevant books and corresponding scores are [1, 3, 2]  [ 2.15656136 -0.04956339 -0.08238967]\n",
      "The most relevant books and corresponding scores are [1, 2, 3]  [ 1.22875045 -0.02028364 -0.02037731]\n",
      "The most relevant books and corresponding scores are [3, 2, 1]  [1.27280758 0.         0.        ]\n",
      "The most relevant books and corresponding scores are [3, 1, 2]  [-0.07273696 -0.07954259 -0.08756572]\n"
     ]
    }
   ],
   "source": [
    "rel = doc_ranking_score(similarities = doc_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Harry Potter and the Philosopher's Stone\""
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval(similarities, documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Full Script\n",
    "--------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def LSI(query):\n",
    "\n",
    "    url_list = [\n",
    "    \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher's%20Stone.txt\",\n",
    "    \"https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/01%20-%20The%20Fellowship%20Of%20The%20Ring.txt\",\n",
    "    \"https://raw.githubusercontent.com/laumann/ds/master/hashing/books/jane-austen-pride-prejudice.txt\"\n",
    "    ]\n",
    "\n",
    "    documents = [\"Harry Potter and the Philosopher's Stone\",\"Fellowship of the Ring\",\"Pride and Prejudice\"]\n",
    "    text = []\n",
    "\n",
    "    for _,i in enumerate(url_list):\n",
    "        req = requests.get(i)\n",
    "        text.append(req.text)\n",
    "\n",
    "    def preprocessing(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "\n",
    "        lower_filter = [w.lower() for w in text]\n",
    "        filtered_text = []\n",
    "\n",
    "        filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "        stop_words = stopwords.words('english') + ['j','page','k','said','rowling','quot','back','mr','mrs']\n",
    "\n",
    "        for words in lower_filter:\n",
    "            if words not in stop_words:\n",
    "                filtered_text.append(words)\n",
    "\n",
    "        filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "        lem = WordNetLemmatizer()\n",
    "\n",
    "        filtered_text = [lem.lemmatize(w) for w in filtered_text]\n",
    "\n",
    "        return filtered_text\n",
    "\n",
    "\n",
    "    def query_processing(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        processed_query = tokenizer.tokenize(text)\n",
    "\n",
    "        processed_query = [w.lower() for w in processed_query]\n",
    "\n",
    "        return processed_query\n",
    "\n",
    "    def doc_ranking_score(similarities):\n",
    "        t = np.argmax(similarities)\n",
    "\n",
    "        return int(t)\n",
    "\n",
    "    processed = [preprocessing(i) for _,i in enumerate(text)]\n",
    "\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import LsiModel\n",
    "\n",
    "    X = Dictionary(processed)\n",
    "    doc_term_matrix = [X.doc2bow(doc) for doc in processed]\n",
    "    lsi_model = LsiModel(corpus=doc_term_matrix,num_topics=3,id2word=X)\n",
    "\n",
    "    processed_query = query_processing(query)\n",
    "\n",
    "    vector_query = lsi_model[Dictionary(processed).doc2bow(processed_query)]\n",
    "\n",
    "    similarities = index[vector_query]\n",
    "\n",
    "    rel = doc_ranking_score(similarities)\n",
    "\n",
    "    return documents[rel]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'Pride and Prejudice'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSI('I Love you Darcy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
