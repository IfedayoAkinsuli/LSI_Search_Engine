{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LSI Implementation for Harry Potter Novels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ifeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parses and stores all 7 texts for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_list = [\n",
    "    \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher's%20Stone.txt\",\"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%202%20-%20The%20Chamber%20of%20Secrets.txt\",\"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\",\"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%204%20-%20The%20Goblet%20of%20Fire.txt\",\"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%205%20-%20The%20Order%20of%20the%20Phoenix.txt\",\n",
    "    \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%206%20-%20The%20Half%20Blood%20Prince.txt\",\n",
    "    \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%207%20-%20The%20Deathly%20Hallows.txt\"\n",
    "    ]\n",
    "\n",
    "text = []\n",
    "\n",
    "for _,i in enumerate(url_list):\n",
    "    req = requests.get(i)\n",
    "    text.append(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    lower_filter = [w.lower() for w in text]\n",
    "    filtered_text = []\n",
    "\n",
    "    filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "    stop_words = stopwords.words('english') + [' j ','page','k','said','rowling','quot','back','mr','mrs']\n",
    "\n",
    "    for words in lower_filter:\n",
    "        if words not in stop_words:\n",
    "            filtered_text.append(words)\n",
    "\n",
    "    filtered_text = [i for i in filtered_text if not i.isdigit()]\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    filtered_text = [lem.lemmatize(w) for w in filtered_text]\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed = [preprocessing(i) for _,i in enumerate(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Query Processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def query_processing(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    processed_query = tokenizer.tokenize(text)\n",
    "\n",
    "    processed_query = [w.lower() for w in processed_query]\n",
    "\n",
    "    return processed_query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "queries = [\"Who is the ghost that haunts the Hogwarts girls' bathroom?\",\"Which animal is the Patronus of Severus Snape?\",\"Who was the Defense Against the Dark Arts teacher in Harry's third year at Hogwarts\",\"Who was the founder of Slytherin House and what was his gifts\",\"What is the name of the spell that causes an object to rise and move according to the caster's will?\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['who',\n 'is',\n 'the',\n 'ghost',\n 'that',\n 'haunts',\n 'the',\n 'hogwarts',\n 'girls',\n 'bathroom']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_queries = [query_processing(i) for _,i in enumerate(queries)]\n",
    "processed_queries[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Bag of Words representing the first book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['boy',\n 'lived',\n 'dursley',\n 'number',\n 'four',\n 'privet',\n 'drive',\n 'proud',\n 'say',\n 'perfectly']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Corpus and Indexing to form Doc-Term-Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "X = Dictionary(processed)\n",
    "\n",
    "doc_term_matrix = [X.doc2bow(doc) for doc in processed]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing the LSI Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsi_model = LsiModel(corpus=doc_term_matrix,num_topics=7,id2word=X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7 Key topics Corresponding with each book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(0,\n '0.721*\"harry\" + 0.208*\"ron\" + 0.199*\"potter\" + 0.185*\"hermione\" + 0.159*\"j\" + 0.117*\"dumbledore\" + 0.098*\"could\" + 0.092*\"know\" + 0.090*\"one\" + 0.082*\"like\"')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics()[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert the Query into LSI Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.03491504088121333), (1, -0.026611678787936324), (2, 0.028591680955758127), (3, -0.021295612037096123), (4, 0.009907197298937059), (5, -0.06193307475850385), (6, -0.007823154196610602)]\n"
     ]
    }
   ],
   "source": [
    "indexed_query = [Dictionary(processed).doc2bow(j) for j in processed_queries]\n",
    "vector_query = [lsi_model[a] for a in indexed_query]\n",
    "print(vector_query[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matrix Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lsi_model[doc_term_matrix])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Ranking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "def doc_ranking_score(similarities):\n",
    "    score = []\n",
    "\n",
    "    for i in similarities:\n",
    "        t = np.argsort(i,-1)[-3:]\n",
    "        sol = i[t]\n",
    "        sol = (list(t+1)[::-1],list(sol)[::-1])\n",
    "        print(f'The most relevant books and corresponding scores are {sol[0]}  {sol[1]}')\n",
    "        score.append(sol[0])\n",
    "\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant books and corresponding scores are [2, 1, 4]  [0.6451366, 0.52452683, 0.47689116]\n",
      "The most relevant books and corresponding scores are [3, 6, 1]  [0.53445834, 0.49757794, 0.4560766]\n",
      "The most relevant books and corresponding scores are [2, 3, 4]  [0.9622794, 0.9406679, 0.9330468]\n",
      "The most relevant books and corresponding scores are [2, 1, 4]  [0.5618626, 0.50057507, 0.3176321]\n",
      "The most relevant books and corresponding scores are [7, 4, 1]  [0.61576915, 0.53625244, 0.51299]\n"
     ]
    }
   ],
   "source": [
    "similarities = index[vector_query]\n",
    "\n",
    "rel = doc_ranking_score(similarities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate key Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "def score(rel):\n",
    "    df = pd.read_csv('Harry_Potter_Query_Scores.csv')\n",
    "\n",
    "    points_available = 6*len(rel)\n",
    "    total = []\n",
    "\n",
    "    for j in range(0,len(rel)):\n",
    "        s = 0\n",
    "        for i in rel[j]:\n",
    "            s+=(df.iloc[j].loc[str(i)])\n",
    "\n",
    "        total.append(s)\n",
    "\n",
    "    return print(f'The total points scored for the LSI is {round(100*(sum(total)/points_available),0)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total points scored for the LSI is 53.0\n"
     ]
    }
   ],
   "source": [
    "score(rel)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Simple BM25 Implementation as a Benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([0.37036529, 0.38783173, 0.33256298, 0.37479915, 0.33736104,\n        0.37829639, 1.67905578]),\n array([0.36118587, 0.34467349, 0.50992329, 0.4042604 , 0.40070315,\n        0.45081909, 0.49288222]),\n array([0.88938352, 0.90023636, 0.91156764, 0.902524  , 0.91000815,\n        0.90566553, 2.20186738]),\n array([0.26182281, 0.35084156, 0.25924586, 0.33936818, 0.3503959 ,\n        0.35996471, 1.65165687]),\n array([0.64635467, 1.55034645, 0.65753762, 0.73481593, 1.3164433 ,\n        1.81848214, 1.45619786])]"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25 = BM25Okapi(processed)\n",
    "doc_scores = [bm25.get_scores(i) for i in processed_queries] # Scores per document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant books and corresponding scores are [7, 2, 6]  [1.679055782916744, 0.3878317283090054, 0.37829638653364905]\n",
      "The most relevant books and corresponding scores are [3, 7, 6]  [0.5099232898673618, 0.4928822244033498, 0.45081908700215223]\n",
      "The most relevant books and corresponding scores are [7, 3, 5]  [2.201867375435711, 0.911567642711484, 0.9100081486408287]\n",
      "The most relevant books and corresponding scores are [7, 6, 2]  [1.6516568701193062, 0.3599647096376914, 0.3508415599113474]\n",
      "The most relevant books and corresponding scores are [6, 2, 7]  [1.81848214442309, 1.5503464451254871, 1.4561978565949074]\n"
     ]
    }
   ],
   "source": [
    "rel = doc_ranking_score(similarities = doc_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total points scored for the LSI is 40.0\n"
     ]
    }
   ],
   "source": [
    "score(rel)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
